# DNS Lookup API with Prometheus Metrics and PostgreSQL

## Overview

This project implements a RESTful API service that provides several DNS-related tools along with Prometheus metrics. The API is designed to be fully containerized using Docker and Docker Compose, ensuring easy setup and deployment. The service includes several endpoints for DNS lookup, IP validation, query history retrieval, and health checks. Additionally, the service is designed with scalability, fault tolerance, and future extensions in mind.

The service is fully documented, and the environment is set up to support modern development workflows using Docker, PostgreSQL, Flask, and Prometheus for monitoring.

---

## Project Structure

The directory structure is as follows:

```
project/
│
├── app/
│   ├── __init__.py        # Application initialization
│   ├── models.py          # Database models
│   ├── routes/            # Contains all route blueprints
│   │   ├── __init__.py    # Route blueprints registration
│   │   ├── health.py      # Health check endpoint
│   │   ├── lookup.py      # DNS lookup endpoint
│   │   ├── validate.py    # IP validation endpoint
│   │   ├── history.py     # DNS query history endpoint
│   └── metrics.py         # Prometheus metrics endpoint
├── migrations/            # Migration files (generated by Flask-Migrate)
├── config.py              # Configuration file (e.g., database credentials)
├── requirements.txt       # Python dependencies
├── Dockerfile             # Dockerfile for containerizing the application
├── docker-compose.yml     # Docker Compose file for orchestration
└── run.py                 # Entry point for starting the Flask application
```

---

## Design Decisions

### 1. **Dockerized Environment**

The entire application is fully Dockerized, meaning both the Flask app and PostgreSQL database are containerized using Docker Compose. This allows for easy setup, isolation of services, and reproducible environments. The use of Docker ensures consistent behavior across different environments (development, testing, production).

The database service is included in `docker-compose.yml` and is set up to run PostgreSQL. The app is connected to this database via environment variables.

### 2. **Prometheus Metrics**

Prometheus metrics are exposed at the `/metrics` endpoint. This was included to enable observability of the API and to monitor key metrics such as:

- Total number of DNS lookups (`dns_lookup_total`)
- Number of successful DNS lookups (`dns_lookup_success`)
- Number of failed DNS lookups (`dns_lookup_failure`)
- Lookup duration histogram (`dns_lookup_duration_seconds`)

These metrics are valuable for monitoring the health and performance of the service.

### 3. **Graceful Shutdown**

The application is configured to handle graceful shutdowns using `signal` and `sys` in Python. This ensures that any running requests are properly terminated and that the service can handle `SIGINT` and `SIGTERM` signals cleanly. This is critical for running in production environments (especially in Kubernetes) where containers are often restarted or terminated.

### 4. **PostgreSQL Database with Migrations**

The database chosen for this project is PostgreSQL due to its robustness, scalability, and wide adoption in production environments. To manage schema changes and database versioning, Flask-Migrate (with Alembic) is used. This ensures a clear migration path for future changes and the ability to handle schema evolutions without downtime.

The database schema includes a `dns_query` table to store DNS lookup results.

### 5. **Security and Scalability**

- **Security**: All DNS queries are logged into the database to ensure traceability. In a production scenario, the API could be extended to include authentication and rate limiting to prevent abuse.
- **Scalability**: The architecture is designed with scalability in mind. With the use of Prometheus metrics and Docker orchestration, it can easily be extended to a microservices-based architecture where multiple instances of the service can run behind a load balancer.
- **Fault Tolerance**: The use of Docker Compose ensures that the entire stack can be spun up with a single command, making it resilient to infrastructure failures.

### 6. **Database Migrations and Initial Setup**

To ensure the database is correctly initialized, follow these steps:

1. **Initialize Migrations**:
   First, create the necessary migration folder:
   ```bash
   docker-compose exec web flask db init
   ```

2. **Create the Initial Migration**:
   Generate the migration script that creates the `dns_query` table:
   ```bash
   docker-compose exec web flask db migrate -m "Initial migration"
   ```

3. **Apply the Migration**:
   Apply the migration to create the tables in the database:
   ```bash
   docker-compose exec web flask db upgrade
   ```

4. **Verify the Table**:
   Connect to the PostgreSQL database to ensure the table has been created:
   ```bash
   docker-compose exec db psql -U postgres -d mydatabase
   ```
   Then run:
   ```sql
   \dt
   ```

---

## Endpoints

- **Root Endpoint (`/`)**: Returns the current date (UNIX epoch), version of the app, and a `kubernetes` boolean indicating whether the app is running in Kubernetes.

- **DNS Lookup (`/v1/tools/lookup`)**: Takes a domain as input and resolves its IPv4 addresses, storing the result in the database. Only successful queries are logged.

- **IP Validation (`/v1/tools/validate`)**: Validates if the provided input is a valid IPv4 address.

- **Query History (`/v1/history`)**: Retrieves the last 20 DNS queries stored in the database.

- **Metrics (`/metrics`)**: Exposes Prometheus metrics for observability.

- **Health Check (`/health`)**: Basic health check endpoint to verify that the service is running.

---

## Quickstart Guide

### Prerequisites

- Docker
- Docker Compose

### Setup Instructions

1. **Clone the Repository**:
   ```bash
   git clone <repository-url>
   cd project
   ```

2. **Build and Start the Services**:
   Build the Docker images and start the services:
   ```bash
   docker-compose up -d --build
   ```

3. **Initialize the Database**:
   After the services have started, initialize the database:
   ```bash
   docker-compose exec web flask db init
   docker-compose exec web flask db migrate -m "Initial migration"
   docker-compose exec web flask db upgrade
   ```

4. **Access the Application**:
   The API will be running at `http://localhost:3000`.

5. **Check Prometheus Metrics**:
   The Prometheus metrics can be accessed at `http://localhost:3000/metrics`.

6. **Verify the Database**:
   Connect to the PostgreSQL database:
   ```bash
   docker-compose exec db psql -U postgres -d mydatabase
   ```

---
## Kubernetes Deployment

Before deploying the application, ensure the required Kubernetes secret is created. I recommend using Sealed Secrets or another secure method to manage sensitive data such as database passwords.

Package the Helm Chart:
```
helm package ./dns-api-helm-chart
```
Install the Helm Chart:

```
helm install dns-api ./dns-api-helm-chart
```

Check the Deployment:
```
kubectl get pods
kubectl get services
```
--
## CI Pipeline

This repository uses GitHub Actions to implement continuous integration, including:

- Running linting checks with `flake8`.
- Running unit tests with `pytest`.
- Building a Docker image and pushing it to Docker Hub.

### GitHub Actions CI Setup:

1. Make sure the following secrets are set in your GitHub repository:
   - `DOCKERHUB_USERNAME`: Your Docker Hub username.
   - `DOCKERHUB_TOKEN`: A personal access token (or password) for Docker Hub.

2. On every commit to the `main` branch, the pipeline will:
   - Run linting and tests.
   - Build the Docker image and push it to Docker Hub.
   - Optionally, package the Helm chart (if Helm is part of the deployment strategy).

To trigger the pipeline, just push your code to the `main` branch or create a pull request targeting the `main` branch.

---

## Conclusion

This project is designed with scalability, fault tolerance, and observability in mind. By leveraging modern tools like Docker, PostgreSQL, and Prometheus, this API is capable of being deployed to production environments such as Kubernetes with minimal effort. The architecture ensures easy setup, fast development cycles, and the ability to extend the service with more features as needed.